{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27904,
     "status": "ok",
     "timestamp": 1576412223852,
     "user": {
      "displayName": "Giulio Bagnoli",
      "photoUrl": "",
      "userId": "16972768245043587235"
     },
     "user_tz": -60
    },
    "id": "k9O3aM3Tb28q",
    "outputId": "3432a258-9afa-48f5-a4ef-4f7005007ce7"
   },
   "outputs": [],
   "source": [
    "!pip3 install 'torch==1.3.1'\n",
    "!pip3 install 'torchvision==0.4.2'\n",
    "!pip3 install 'Pillow-SIMD'\n",
    "!pip3 install 'tqdm'\n",
    "!pip3 install  'sklearn'  \n",
    "!pip3 install --upgrade 'pillow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fo942LMOdlh4"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DokFOdD1dJEl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "import copy \n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import alexnet\n",
    "from torchvision.models import vgg11\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.datasets import VisionDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OIDLJuIXK_vh"
   },
   "source": [
    "**Set Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5PkYfqfK_SA"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'                # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 101              # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 256               # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                               # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LRS = [0.1, 0.01, 0.01, 0.001 ]                  # The initial Learning Rate\n",
    "MOMENTUM = 0.9                                   # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-5                              # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 200                        # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [20, 30, 45, 60]             # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1                              # Multiplicative factor for learning rate step-down\n",
    "\n",
    "\n",
    "#Values to early stopping\n",
    "HP = 5                    \n",
    "strip_len = 5\n",
    "\n",
    "# Define transforms for training phase\n",
    "train_transform = transforms.Compose([transforms.Resize(256),      \n",
    "                                      transforms.CenterCrop(224),  \n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "# Define transforms for the alexnet pretrained\n",
    "TL_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                    \n",
    "])\n",
    "\n",
    "# Define tranforms for the data augmentation\n",
    "data_augmentation_transform_1 = transforms.Compose([transforms.Resize(256), \n",
    "                                      transforms.RandomCrop(224),  \n",
    "                                      transforms.Grayscale(3),  \n",
    "                                      transforms.RandomHorizontalFlip(),          \n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  \n",
    "])\n",
    "\n",
    "data_augmentation_transform_2 = transforms.Compose([transforms.Resize(256),      \n",
    "                                      transforms.Grayscale(3),  \n",
    "                                      transforms.RandomCrop(224),  \n",
    "                                      transforms.RandomVerticalFlip(),          \n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  \n",
    "])\n",
    "\n",
    "data_augmentation_transform_3 = transforms.Compose([transforms.Resize(256),      \n",
    "                                      transforms.RandomCrop(224),  \n",
    "                                      transforms.RandomHorizontalFlip(),          \n",
    "                                      transforms.ColorJitter(),                                                                                \n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blhYbLkE_6tn"
   },
   "source": [
    "**Utility**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-4B4K4r_u0u"
   },
   "outputs": [],
   "source": [
    "def makePlot(x, ys, xLabel, yLabel, legend):\n",
    "    for y in ys:\n",
    "        plt.plot(x, y)\n",
    "    plt.xlabel(xLabel)\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.legend(legend)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    img = Image.open(open(path, 'rb'), mode='r')\n",
    "    return img.convert('RGB')\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    if not ((extensions is None) ^ (is_valid_file is None)):\n",
    "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
    "    if extensions is not None:\n",
    "        def is_valid_file(x):\n",
    "            return has_file_allowed_extension(x, extensions)\n",
    "    for target in sorted(class_to_idx.keys()):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        for root, _, fnames in sorted(os.walk(d, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = os.path.join(root, fname)\n",
    "                if is_valid_file(path):\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def has_file_allowed_extension(filename, extensions):\n",
    "    \"\"\"Checks if a file is an allowed extension.\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "        extensions (tuple of strings): extensions to consider (lowercase)\n",
    "    Returns:\n",
    "        bool: True if the filename ends with one of given extensions\n",
    "    \"\"\"\n",
    "    return filename.lower().endswith(extensions)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def prepareDataloader(transform):\n",
    "    # Clone github repository with data\n",
    "    if not os.path.isdir('./Homework2-Caltech101'):\n",
    "        !git\n",
    "        clone\n",
    "        https: // github.com / MachineLearning2020 / Homework2 - Caltech101.git\n",
    "\n",
    "    DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n",
    "\n",
    "    # Prepare Pytorch Datasets\n",
    "    dataset = Caltech(DATA_DIR, transform=transform)\n",
    "\n",
    "    train_indexes = []\n",
    "    test_indexes = []\n",
    "\n",
    "    test_path = open(\"Homework2-Caltech101/test.txt\", \"r\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "\n",
    "for name in test_path.readlines():\n",
    "    names.append(name[:-1])\n",
    "\n",
    "num_lines = len(open(\"Homework2-Caltech101/test.txt\").readlines())\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    test = False\n",
    "    for name in names:\n",
    "        if (DATA_DIR + \"/\" + name) == dataset.samples[index][0]:\n",
    "            test_indexes.append(index)\n",
    "            test = True\n",
    "            test_path.seek(0)\n",
    "            break\n",
    "test_path.seek(0)\n",
    "\n",
    "if not test:\n",
    "    train_indexes.append(index)\n",
    "\n",
    "train_val_dataset = Subset(dataset, train_indexes)\n",
    "test_dataset = Subset(dataset, test_indexes)\n",
    "\n",
    "train_dataset = []\n",
    "val_dataset = []\n",
    "\n",
    "# Essendo gli elementi ordinati per classe va bene anche questo split\n",
    "for index in range(len(train_val_dataset)):\n",
    "    if (index % 2) == 0:\n",
    "        train_dataset.append(train_val_dataset[index])\n",
    "    else:\n",
    "        val_dataset.append(train_val_dataset[index])\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Val Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))\n",
    "print('Train + Val dataset: {}'.format(len(train_val_dataset)))\n",
    "\n",
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "train_val_dataloader = DataLoader(train_val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "return train_dataloader, val_dataloader, test_dataloader, train_val_dataloader\n",
    "\n",
    "\n",
    "def stoppingCriteria(accuracy, last_acc_val, hp, early_stopping, epoch):\n",
    "    if accuracy > last_acc_val:\n",
    "        hp = HP\n",
    "        aux_str = \"\"\n",
    "    else:\n",
    "        hp -= 1\n",
    "        aux_str = \" Accuracy has not encreased, hp=\" + str(hp)\n",
    "\n",
    "    if epoch >= (strip_len * HP) and hp == 0:\n",
    "        early_stopping = True\n",
    "\n",
    "        last_acc_val = accuracy\n",
    "        return aux_str, early_stopping, hp, last_acc_val\n",
    "\n",
    "\n",
    "def tuneHyperparameters(train_dataloader, val_dataloader, test_dataloader, train_val_dataloader, input_net):\n",
    "    ##############################\n",
    "    #   Tuning Hyperparameters   #\n",
    "    ##############################\n",
    "\n",
    "    top_model = {'lr': 0, 'num_epoch': 0, 'acc': 0, 'step_size': 0}\n",
    "\n",
    "    for i in range(len(LRS)):\n",
    "        print(\"Starting (LR=\" + str(LRS[i]) + \", step size=\" + str(STEP_SIZE[i]) + \").\")\n",
    "\n",
    "        net = copy.deepcopy(input_net)\n",
    "        net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        parameters_to_optimize = net.parameters()\n",
    "\n",
    "        optimizer = optim.SGD(parameters_to_optimize, lr=LRS[i], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE[i], gamma=GAMMA)\n",
    "\n",
    "        net = net.to(DEVICE)\n",
    "        epoch = 0\n",
    "        early_stopping = False\n",
    "        hp = copy.deepcopy(HP)\n",
    "        last_acc_val = 0\n",
    "        top_val_partial = {'acc': 0, 'epoch': 0}\n",
    "\n",
    "    while (epoch < NUM_EPOCHS and early_stopping == False):\n",
    "        epoch += 1\n",
    "\n",
    "        for images, labels in train_dataloader:\n",
    "\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            net.train()\n",
    "            optimizer.zero_grad()  # Zero-ing the gradients\n",
    "\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if math.isnan(loss):\n",
    "                print(\"           loss:\", loss)\n",
    "                early_stopping = True\n",
    "\n",
    "            loss.backward()  # backward pass: computes gradients\n",
    "            optimizer.step()  # update weights based on accumulated gradients\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        ################################################\n",
    "        #   Evaluate the model on the validation set   #\n",
    "        ################################################\n",
    "\n",
    "    net.train(False)  # Set Network to evaluation mode\n",
    "    running_corrects = 0\n",
    "\n",
    "    for images, labels in val_dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = net(images)\n",
    "\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update Corrects\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    accuracy = running_corrects / float(len(val_dataloader.dataset))\n",
    "\n",
    "    if accuracy > top_model['acc']:\n",
    "        top_model['acc'] = accuracy\n",
    "        top_model['lr'] = LRS[i]\n",
    "        top_model['num_epoch'] = epoch\n",
    "        top_model['step_size'] = STEP_SIZE[i]\n",
    "\n",
    "    aux_str = \"\"\n",
    "    if epoch % strip_len == 0:\n",
    "        aux_str, early_stopping, hp, last_acc_val = stoppingCriteria(accuracy, last_acc_val, hp, early_stopping, epoch)\n",
    "\n",
    "    if top_val_partial['acc'] < accuracy:\n",
    "        top_val_partial['acc'] = accuracy\n",
    "        top_val_partial['epoch'] = epoch\n",
    "\n",
    "    print(\" Ending epoch \" + str(epoch) + \"/\" + str(NUM_EPOCHS) + \", LR = \" + str(\n",
    "        scheduler.get_lr()) + \", step size = \" + str(STEP_SIZE[i]) + \", Accuracy on validation set = \" + str(\n",
    "        round(accuracy * 100, 4)) + aux_str)\n",
    "\n",
    "\n",
    "print(\"Best accuracy: \", round(top_val_partial['acc'] * 100, 4), \"%. Epoch: \", top_val_partial['epoch'], \".\")\n",
    "\n",
    "print(\"The set of the best hyperparameters is: [lr = \" + str(top_model[\"lr\"]) +\n",
    "      \", num_epoch = \" + str(top_model['num_epoch']) + \", step size = \" +\n",
    "      str(top_model['step_size']) + \" ]. The best accuracy is: \" +\n",
    "      str(round(top_model['acc'] * 100, 2)) + \"%.\")\n",
    "\n",
    "#############################\n",
    "#   Train net on train+val  #\n",
    "#############################\n",
    "\n",
    "print(\n",
    "    \"----------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Strart train net on train+val\")\n",
    "\n",
    "net = copy.deepcopy(input_net)\n",
    "net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters_to_optimize = net.parameters()\n",
    "\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=top_model['lr'], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=top_model['step_size'], gamma=GAMMA)\n",
    "\n",
    "net = net.to(DEVICE)\n",
    "\n",
    "for epoch in range(top_model['num_epoch']):\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        net.train()\n",
    "        optimizer.zero_grad()  # Zero-ing the gradients\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if math.isnan(loss):\n",
    "            print(\"           loss:\", loss)\n",
    "\n",
    "        loss.backward()  # backward pass: computes gradients\n",
    "        optimizer.step()  # update weights based on accumulated gradients\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\" Ending epoch \" + str(epoch + 1) + \"/\" + str(top_model['num_epoch']) + \", LR = \", str(scheduler.get_lr()),\n",
    "          \", step size = \", top_model['step_size'])\n",
    "\n",
    "#########################\n",
    "#   Evaluate Test Set   #\n",
    "#########################\n",
    "\n",
    "print(\n",
    "    \"----------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Strart test net on test\")\n",
    "net = net.to(DEVICE)  # this will bring the network to GPU if DEVICE is cuda\n",
    "net.train(False)  # Set Network to evaluation mode\n",
    "\n",
    "running_corrects = 0\n",
    "for images, labels in tqdm(test_dataloader):\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    # Forward Pass\n",
    "    outputs = net(images)\n",
    "\n",
    "    # Get predictions\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Update Corrects\n",
    "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = running_corrects / float(len(test_dataloader.dataset))\n",
    "\n",
    "print(\"Test Accuracy: \" + str(round(accuracy * 100, 2)) + \"%.\")\n",
    "\n",
    "return top_model\n",
    "\n",
    "\n",
    "def partialTrain(train_val_dataloader, test_dataloader, top_model_full_opt, opt):\n",
    "    top_model = {'net': 0, 'num_epoch': 0, 'acc': 0}\n",
    "    print(\"Starting (LR=\" + str(top_model_full_opt[\"lr\"]) + \", step size=\" + str(\n",
    "        top_model_full_opt[\"step_size\"]) + \", num max epoch: \" + str(top_model_full_opt[\"num_epoch\"]) + \").\")\n",
    "\n",
    "    net = alexnet(pretrained=True)\n",
    "    net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if opt == \"FC\":\n",
    "        parameters_to_optimize = net.classifier.parameters()\n",
    "        parameters_to_not_optimize = net.features.parameters()\n",
    "        for p in parameters_to_not_optimize:\n",
    "            p.requires_grad = False\n",
    "    else:\n",
    "        parameters_to_optimize = net.features.parameters()\n",
    "        parameters_to_not_optimize = net.classifier.parameters()\n",
    "        for p in parameters_to_not_optimize:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    optimizer = optim.SGD(parameters_to_optimize, lr=top_model_full_opt[\"lr\"], momentum=MOMENTUM,\n",
    "                          weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=top_model_full_opt[\"step_size\"], gamma=GAMMA)\n",
    "\n",
    "    net = net.to(DEVICE)\n",
    "\n",
    "    for epoch in range(top_model_full_opt['num_epoch']):\n",
    "\n",
    "        ##############################\n",
    "        #   Train Net on train+val   #\n",
    "        ##############################\n",
    "\n",
    "        for images, labels in train_val_dataloader:\n",
    "\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            net.train()\n",
    "            optimizer.zero_grad()  # Zero-ing the gradients\n",
    "\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if math.isnan(loss):\n",
    "                print(\"           loss:\", loss)\n",
    "\n",
    "            loss.backward()  # backward pass: computes gradients\n",
    "            optimizer.step()  # update weights based on accumulated gradients\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\" Ending epoch \" + str(epoch) + \"/\" + str(top_model_full_opt['num_epoch']) + \", LR = \",\n",
    "              top_model_full_opt['lr'], \", step size = \", top_model_full_opt['step_size'])\n",
    "\n",
    "    #########################\n",
    "    #   Evaluate Test Set   #\n",
    "    #########################\n",
    "\n",
    "    net = net.to(DEVICE)  # this will bring the network to GPU if DEVICE is cuda\n",
    "    net.train(False)  # Set Network to evaluation mode\n",
    "\n",
    "    running_corrects = 0\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = net(images)\n",
    "\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update Corrects\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    accuracy = running_corrects / float(len(test_dataloader.dataset))\n",
    "\n",
    "    print(\"Test Accuracy: \" + str(round(accuracy * 100, 2)) + \"%.\")\n",
    "\n",
    "\n",
    "class Caltech(VisionDataset):\n",
    "\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None):\n",
    "\n",
    "        super(Caltech, self).__init__(root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "        classes, class_to_idx = self._find_classes(self.root)\n",
    "        samples = make_dataset(self.root, class_to_idx, IMG_EXTENSIONS)\n",
    "\n",
    "        if len(samples) == 0:\n",
    "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
    "                                                                                 \"Supported extensions are: \" + \",\".join(\n",
    "                IMG_EXTENSIONS)))\n",
    "\n",
    "        self.split = split  # This defines the split you are going to use\n",
    "        # (split files are called 'train.txt' and 'test.txt')\n",
    "        self.loader = default_loader\n",
    "        self.extensions = IMG_EXTENSIONS\n",
    "\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.samples = samples\n",
    "        self.targets = [s[1] for s in samples]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        classes.remove(\"BACKGROUND_Google\")\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Provide a way to access image and label via index. Image should be a PIL Image  label can be int\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len  # Provide a way to get the length (number of elements) of the dataset\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9AOBZPb9IWvd"
   },
   "source": [
    "**Punti 1, 2**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 299639,
     "status": "ok",
     "timestamp": 1576412495622,
     "user": {
      "displayName": "Giulio Bagnoli",
      "photoUrl": "",
      "userId": "16972768245043587235"
     },
     "user_tz": -60
    },
    "id": "louCvsGLIUrw",
    "outputId": "08f9a4cd-df40-4e2d-f2f0-eb3380a2fee9"
   },
   "outputs": [],
   "source": [
    "%xmode Verbose\n",
    "train_dataloader, val_dataloader, test_dataloader, train_val_dataloader = prepareDataloader(train_transform)\n",
    "net = alexnet() \n",
    "tuneHyperparameters(train_dataloader, val_dataloader, test_dataloader, train_val_dataloader, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnN9zTe2UThq"
   },
   "source": [
    "**Transfert Learning**\n",
    "Punto 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 798862,
     "status": "ok",
     "timestamp": 1576412994858,
     "user": {
      "displayName": "Giulio Bagnoli",
      "photoUrl": "",
      "userId": "16972768245043587235"
     },
     "user_tz": -60
    },
    "id": "Ca_WDs7PUTKr",
    "outputId": "6a389107-aea4-4e48-a5ab-9805d52f1ee9"
   },
   "outputs": [],
   "source": [
    "%xmode Verbose\n",
    "#################\n",
    "#   Punto 3.A   #\n",
    "#################\n",
    "net = alexnet(pretrained = True)\n",
    "\n",
    "\n",
    "#################\n",
    "#   Punto 3.B   #\n",
    "#################\n",
    "train_dataloader_TL, val_dataloader_TL, test_dataloader_TL, train_val_dataloader_TL = prepareDataloader(TL_transform)\n",
    "\n",
    "\n",
    "#################\n",
    "#   Punto 3.C   #\n",
    "#################\n",
    "print(\"#################\")\n",
    "print(\"#   Punto 3.C   #\")\n",
    "print(\"#################\")\n",
    "top_model_full_opt = tuneHyperparameters(train_dataloader_TL, val_dataloader_TL, test_dataloader_TL, train_val_dataloader_TL, net)\n",
    "\n",
    "\n",
    "#################\n",
    "#   Punto 3.D   #\n",
    "#################\n",
    "print(\"#################\")\n",
    "print(\"#   Punto 3.D   #\")\n",
    "print(\"#################\")\n",
    "partialTrain(train_val_dataloader_TL, test_dataloader_TL, top_model_full_opt, \"FC\")\n",
    "\n",
    "\n",
    "#################\n",
    "#   Punto 3.E   #\n",
    "#################\n",
    "print(\"#################\")\n",
    "print(\"#   Punto 3.E   #\")\n",
    "print(\"#################\")\n",
    "partialTrain(train_val_dataloader_TL, test_dataloader_TL, top_model_full_opt, \"CL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXLhAJ8jSJ0E"
   },
   "source": [
    "**Punto 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1542155,
     "status": "ok",
     "timestamp": 1576413738155,
     "user": {
      "displayName": "Giulio Bagnoli",
      "photoUrl": "",
      "userId": "16972768245043587235"
     },
     "user_tz": -60
    },
    "id": "OiuRqfbUSKKT",
    "outputId": "2f8334f2-177b-410d-d4ef-0d642c658337"
   },
   "outputs": [],
   "source": [
    "\n",
    "%xmode Verbose\n",
    "train_dataloader_DA_1, val_dataloader_DA_1, test_dataloader_DA_1, train_val_dataloader_DA_1 = prepareDataloader(data_augmentation_transform_1)\n",
    "train_dataloader_DA_2, val_dataloader_DA_2, test_dataloader_DA_2, train_val_dataloader_DA_2 = prepareDataloader(data_augmentation_transform_2)\n",
    "train_dataloader_DA_3, val_dataloader_DA_3, test_dataloader_DA_3, train_val_dataloader_DA_3 = prepareDataloader(data_augmentation_transform_3)\n",
    "\n",
    "net = alexnet(pretrained = True)\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "print(\"First DA\")\n",
    "tuneHyperparameters(train_dataloader_DA_1, val_dataloader, test_dataloader, train_val_dataloader_DA_1, net) \n",
    "print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "print(\"Second DA\")\n",
    "tuneHyperparameters(train_dataloader_DA_2, val_dataloader, test_dataloader, train_val_dataloader_DA_2, net) \n",
    "print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "print(\"Third DA\")\n",
    "tuneHyperparameters(train_dataloader_DA_3, val_dataloader, test_dataloader, train_val_dataloader_DA_3, net) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1cDK-yFCQeq"
   },
   "source": [
    "Punto 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7CJlF8rvCSoB",
    "outputId": "dba15bb7-58f0-462f-cbca-3151d473ea2e"
   },
   "outputs": [],
   "source": [
    "%xmode Verbose\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "print(\"#################\")\n",
    "print(\"# Punto 5.1/5.2 #\")\n",
    "print(\"#################\")\n",
    "\n",
    "net = vgg11() \n",
    "train_dataloader, val_dataloader, test_dataloader, train_val_dataloader = prepareDataloader(train_transform)\n",
    "tuneHyperparameters(train_dataloader, val_dataloader, test_dataloader, train_val_dataloader, net)\n",
    "\n",
    "\n",
    "#################\n",
    "#   Punto 5.3   #\n",
    "#################\n",
    "\n",
    "print(\"#################\")\n",
    "print(\"#   Punto 5.3   #\")\n",
    "print(\"#################\")\n",
    "\n",
    "net = vgg11(pretrained = True)\n",
    "train_dataloader_TL, val_dataloader_TL, test_dataloader_TL, train_val_dataloader_TL = prepareDataloader(TL_transform)\n",
    "top_model_full_opt = tuneHyperparameters(train_dataloader_TL, val_dataloader_TL, test_dataloader_TL, train_val_dataloader_TL, net)\n",
    "\n",
    "\n",
    "#################\n",
    "#   Punto 5.4   #\n",
    "#################\n",
    "\n",
    "print(\"#################\")\n",
    "print(\"#   Punto 5.4   #\")\n",
    "print(\"#################\")\n",
    "\n",
    "train_dataloader_DA_1, val_dataloader_DA_1, test_dataloader_DA_1, train_val_dataloader_DA_1 = prepareDataloader(data_augmentation_transform_1)\n",
    "train_dataloader_DA_2, val_dataloader_DA_2, test_dataloader_DA_2, train_val_dataloader_DA_2 = prepareDataloader(data_augmentation_transform_2)\n",
    "train_dataloader_DA_3, val_dataloader_DA_3, test_dataloader_DA_3, train_val_dataloader_DA_3 = prepareDataloader(data_augmentation_transform_3)\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "print(\"First DA\")\n",
    "tuneHyperparameters(train_dataloader_DA_1, val_dataloader, test_dataloader, train_val_dataloader_DA_1, net) \n",
    "print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "print(\"Second DA\")\n",
    "tuneHyperparameters(train_dataloader_DA_2, val_dataloader, test_dataloader, train_val_dataloader_DA_2, net) \n",
    "print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "print(\"Third DA\")\n",
    "tuneHyperparameters(train_dataloader_DA_3, val_dataloader, test_dataloader, train_val_dataloader_DA_3, net) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "homework2.ipynb",
   "provenance": [
    {
     "file_id": "1PhNPpklp9FbxJEtsZ8Jp9qXQa4aZDK5Y",
     "timestamp": 1574959458760
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
