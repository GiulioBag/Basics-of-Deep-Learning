{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"homework2.ipynb","provenance":[{"file_id":"1PhNPpklp9FbxJEtsZ8Jp9qXQa4aZDK5Y","timestamp":1574959458760}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"k9O3aM3Tb28q","colab_type":"code","outputId":"3432a258-9afa-48f5-a4ef-4f7005007ce7","executionInfo":{"status":"ok","timestamp":1576412223852,"user_tz":-60,"elapsed":27904,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["!pip3 install 'torch==1.3.1'\n","!pip3 install 'torchvision==0.4.2'\n","!pip3 install 'Pillow-SIMD'\n","!pip3 install 'tqdm'\n","!pip3 install  'sklearn'  \n","!pip3 install --upgrade 'pillow'"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.17.4)\n","Requirement already satisfied: torchvision==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n","Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.17.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (6.2.1)\n","Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (6.0.0.post0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.3)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n","Requirement already up-to-date: pillow in /usr/local/lib/python3.6/dist-packages (6.2.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fo942LMOdlh4","colab_type":"text"},"source":["**Import libraries**"]},{"cell_type":"code","metadata":{"id":"DokFOdD1dJEl","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","import sys\n","import io\n","import copy \n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Subset, DataLoader\n","from torch.backends import cudnn\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.models import alexnet\n","from torchvision.models import vgg11\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","from torchvision.datasets import VisionDataset\n","\n","from sklearn.model_selection import train_test_split\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OIDLJuIXK_vh","colab_type":"text"},"source":["**Set Arguments**"]},{"cell_type":"code","metadata":{"id":"d5PkYfqfK_SA","colab_type":"code","colab":{}},"source":["DEVICE = 'cuda'                # 'cuda' or 'cpu'\n","\n","NUM_CLASSES = 101              # 101 + 1: There is am extra Background class that should be removed \n","\n","BATCH_SIZE = 256               # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n","                               # the batch size, learning rate should change by the same factor to have comparable results\n","\n","LRS = [0.1, 0.01, 0.01, 0.001 ]                  # The initial Learning Rate\n","MOMENTUM = 0.9                                   # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5                              # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 200                        # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = [20, 30, 45, 60]             # How many epochs before decreasing learning rate (if using a step-down policy)\n","GAMMA = 0.1                              # Multiplicative factor for learning rate step-down\n","\n","\n","#Values to early stopping\n","HP = 5                    \n","strip_len = 5\n","\n","# Define transforms for training phase\n","train_transform = transforms.Compose([transforms.Resize(256),      \n","                                      transforms.CenterCrop(224),  \n","                                      transforms.ToTensor(), \n","                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n","])\n","\n","# Define transforms for the alexnet pretrained\n","TL_transform = transforms.Compose([transforms.Resize(256),\n","                                      transforms.CenterCrop(224),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                    \n","])\n","\n","# Define tranforms for the data augmentation\n","data_augmentation_transform_1 = transforms.Compose([transforms.Resize(256), \n","                                      transforms.RandomCrop(224),  \n","                                      transforms.Grayscale(3),  \n","                                      transforms.RandomHorizontalFlip(),          \n","                                      transforms.ToTensor(), \n","                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  \n","])\n","\n","data_augmentation_transform_2 = transforms.Compose([transforms.Resize(256),      \n","                                      transforms.Grayscale(3),  \n","                                      transforms.RandomCrop(224),  \n","                                      transforms.RandomVerticalFlip(),          \n","                                      transforms.ToTensor(), \n","                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  \n","])\n","\n","data_augmentation_transform_3 = transforms.Compose([transforms.Resize(256),      \n","                                      transforms.RandomCrop(224),  \n","                                      transforms.RandomHorizontalFlip(),          \n","                                      transforms.ColorJitter(),                                                                                \n","                                      transforms.ToTensor(), \n","                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  \n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blhYbLkE_6tn","colab_type":"text"},"source":["**Utility**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"w-4B4K4r_u0u","colab_type":"code","colab":{}},"source":["def makePlot(x, ys, xLabel, yLabel, legend):\n","  for y in ys:\n","    plt.plot(x, y)\n","  plt.xlabel(xLabel)\n","  plt.ylabel(yLabel)\n","  plt.legend(legend)\n","  plt.show()\n","\n","def pil_loader(path):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    img = Image.open(open(path, 'rb'), mode='r')\n","    return img.convert('RGB')\n","\n","def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n","    images = []\n","    dir = os.path.expanduser(dir)\n","    if not ((extensions is None) ^ (is_valid_file is None)):\n","        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n","    if extensions is not None:\n","        def is_valid_file(x):\n","            return has_file_allowed_extension(x, extensions)\n","    for target in sorted(class_to_idx.keys()):\n","        d = os.path.join(dir, target)\n","        if not os.path.isdir(d):\n","            continue\n","        for root, _, fnames in sorted(os.walk(d, followlinks=True)):\n","            for fname in sorted(fnames):\n","                path = os.path.join(root, fname)\n","                if is_valid_file(path):\n","                    item = (path, class_to_idx[target])\n","                    images.append(item)\n","\n","    return images\n","\n","def has_file_allowed_extension(filename, extensions):\n","    \"\"\"Checks if a file is an allowed extension.\n","    Args:\n","        filename (string): path to a file\n","        extensions (tuple of strings): extensions to consider (lowercase)\n","    Returns:\n","        bool: True if the filename ends with one of given extensions\n","    \"\"\"\n","    return filename.lower().endswith(extensions)\n","\n","def default_loader(path):\n","    from torchvision import get_image_backend\n","    if get_image_backend() == 'accimage':\n","        return accimage_loader(path)\n","    else:\n","        return pil_loader(path)\n","\n","def prepareDataloader(transform):\n","  # Clone github repository with data\n","  if not os.path.isdir('./Homework2-Caltech101'):\n","    !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n","\n","  DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n","\n","  # Prepare Pytorch Datasets\n","  dataset = Caltech(DATA_DIR, transform=transform)\n","\n","  train_indexes = []\n","  test_indexes = []\n","\n","  test_path = open(\"Homework2-Caltech101/test.txt\", \"r\")\n","\n","  names = []\n","  for name in test_path.readlines():\n","    names.append(name[:-1])\n","\n","  num_lines = len(open(\"Homework2-Caltech101/test.txt\").readlines(  ))\n","\n","  for index in range(len(dataset)):\n","    test = False\n","    for name in names:\n","      if (DATA_DIR + \"/\" + name) == dataset.samples[index][0]:\n","        test_indexes.append(index)\n","        test = True\n","        test_path.seek(0)\n","        break\n","      test_path.seek(0)\n","\n","    if not test:\n","      train_indexes.append(index)\n","\n","  train_val_dataset = Subset(dataset, train_indexes)\n","  test_dataset = Subset(dataset, test_indexes)\n","\n","  train_dataset = []\n","  val_dataset = []\n","\n","  # Essendo gli elementi ordinati per classe va bene anche questo split\n","  for index in range(len(train_val_dataset)):\n","    if (index%2) == 0:\n","      train_dataset.append(train_val_dataset[index])\n","    else:\n","      val_dataset.append(train_val_dataset[index])\n","\n","\n","  # Check dataset sizes\n","  print('Train Dataset: {}'.format(len(train_dataset)))\n","  print('Val Dataset: {}'.format(len(val_dataset)))\n","  print('Test Dataset: {}'.format(len(test_dataset)))\n","  print('Train + Val dataset: {}'.format(len(train_val_dataset)))\n","\n","  # Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n","  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","  train_val_dataloader = DataLoader(train_val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","\n","  return train_dataloader, val_dataloader, test_dataloader, train_val_dataloader\n","\n","def stoppingCriteria(accuracy, last_acc_val, hp, early_stopping, epoch):\n","\n","  if accuracy > last_acc_val:\n","    hp = HP\n","    aux_str= \"\"\n","  else:\n","    hp -= 1\n","    aux_str = \" Accuracy has not encreased, hp=\" + str(hp)\n","\n","  if epoch >= (strip_len * HP) and hp == 0:\n","    early_stopping = True\n","\n","  last_acc_val = accuracy\n","  return aux_str, early_stopping, hp, last_acc_val\n","\n","def tuneHyperparameters(train_dataloader, val_dataloader, test_dataloader, train_val_dataloader, input_net):\n","    \n","  ##############################\n","  #   Tuning Hyperparameters   #\n","  ##############################\n","\n","  top_model = { 'lr':0, 'num_epoch': 0, 'acc': 0, 'step_size': 0}\n","\n","  for i in range(len(LRS)):  \n","    print(\"Starting (LR=\" + str(LRS[i]) + \", step size=\" + str(STEP_SIZE[i]) + \").\")\n","\n","    net = copy.deepcopy(input_net)\n","    net.classifier[6] = nn.Linear(4096, NUM_CLASSES) \n","\n","    criterion = nn.CrossEntropyLoss()\n","    parameters_to_optimize = net.parameters() \n","\n","    optimizer = optim.SGD(parameters_to_optimize, lr=LRS[i], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE[i], gamma=GAMMA)\n","\n","    net = net.to(DEVICE)\n","    epoch = 0 \n","    early_stopping = False\n","    hp = copy.deepcopy(HP)\n","    last_acc_val = 0\n","    top_val_partial = {'acc': 0, 'epoch': 0}\n","\n","    while (epoch < NUM_EPOCHS and early_stopping == False):\n","      epoch += 1\n","\n","      for images, labels in train_dataloader:\n","        \n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        net.train()\n","        optimizer.zero_grad() # Zero-ing the gradients\n","\n","        outputs = net(images)\n","        loss = criterion(outputs, labels)\n","        if math.isnan(loss):\n","          print(\"           loss:\",loss)\n","          early_stopping = True\n","\n","        loss.backward()  # backward pass: computes gradients\n","        optimizer.step() # update weights based on accumulated gradients\n","\n","      scheduler.step() \n","\n","\n","  ################################################\n","  #   Evaluate the model on the validation set   #\n","  ################################################\n","\n","      net.train(False) # Set Network to evaluation mode\n","      running_corrects = 0\n","\n","      for images, labels in val_dataloader: \n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        # Forward Pass\n","        outputs = net(images)\n","\n","        # Get predictions\n","        _, preds = torch.max(outputs.data, 1)\n","\n","        # Update Corrects\n","        running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","      # Calculate Accuracy\n","      accuracy = running_corrects / float(len(val_dataloader.dataset))\n","\n","      if accuracy > top_model['acc']:\n","        top_model['acc'] = accuracy\n","        top_model['lr'] = LRS[i]\n","        top_model['num_epoch'] = epoch\n","        top_model['step_size'] = STEP_SIZE[i]\n","      \n","      aux_str = \"\"\n","      if epoch % strip_len == 0:\n","        aux_str, early_stopping, hp, last_acc_val = stoppingCriteria(accuracy, last_acc_val, hp, early_stopping, epoch)\n","\n","      if top_val_partial['acc'] < accuracy:\n","        top_val_partial['acc'] = accuracy\n","        top_val_partial ['epoch'] = epoch\n","      \n","\n","      print(\" Ending epoch \" + str(epoch)+ \"/\" + str(NUM_EPOCHS) + \", LR = \" + str(scheduler.get_lr()) + \", step size = \" + str(STEP_SIZE[i]) + \", Accuracy on validation set = \" + str(round(accuracy*100, 4)) + aux_str)\n","    \n","    print(\"Best accuracy: \", round(top_val_partial['acc'] *100, 4), \"%. Epoch: \", top_val_partial['epoch'], \".\")\n","\n","\n","  print(\"The set of the best hyperparameters is: [lr = \" + str(top_model[\"lr\"]) +\n","      \", num_epoch = \" + str(top_model['num_epoch']) + \", step size = \" + \n","      str(top_model['step_size']) + \" ]. The best accuracy is: \" + \n","      str(round(top_model['acc']*100, 2)) + \"%.\" )\n","  \n","  #############################\n","  #   Train net on train+val  #\n","  #############################\n","\n","  print(\"----------------------------------------------------------------------------------------------------------------------------------------------\")\n","  print(\"Strart train net on train+val\")\n","\n","  net = copy.deepcopy(input_net)\n","  net.classifier[6] = nn.Linear(4096, NUM_CLASSES) \n","\n","  criterion = nn.CrossEntropyLoss()\n","  parameters_to_optimize = net.parameters() \n","\n","  optimizer = optim.SGD(parameters_to_optimize, lr=top_model['lr'], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=top_model['step_size'], gamma=GAMMA)\n","\n","  net = net.to(DEVICE)\n","\n","  for epoch in range(top_model['num_epoch']):\n","\n","      for images, labels in train_dataloader:\n","        \n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        net.train()\n","        optimizer.zero_grad() # Zero-ing the gradients\n","\n","        outputs = net(images)\n","        loss = criterion(outputs, labels)\n","        if math.isnan(loss):\n","          print(\"           loss:\", loss)\n","\n","        loss.backward()  # backward pass: computes gradients\n","        optimizer.step() # update weights based on accumulated gradients\n","\n","      scheduler.step() \n","      print(\" Ending epoch \" + str(epoch + 1) + \"/\" + str(top_model['num_epoch']) + \", LR = \", str(scheduler.get_lr()), \", step size = \", top_model['step_size'])\n","\n","\n","  #########################\n","  #   Evaluate Test Set   #\n","  #########################\n","\n","  print(\"----------------------------------------------------------------------------------------------------------------------------------------------\")\n","  print(\"Strart test net on test\")\n","  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","  net.train(False) # Set Network to evaluation mode\n","\n","  running_corrects = 0\n","  for images, labels in tqdm(test_dataloader): \n","    images = images.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","\n","    # Forward Pass\n","    outputs = net(images)\n","\n","    # Get predictions\n","    _, preds = torch.max(outputs.data, 1)\n","\n","    # Update Corrects\n","    running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","  # Calculate Accuracy\n","  accuracy = running_corrects / float(len(test_dataloader.dataset))\n","\n","  print(\"Test Accuracy: \" + str(round(accuracy*100, 2)) + \"%.\" )\n","\n","  return top_model\n","\n","\n","def partialTrain(train_val_dataloader,  test_dataloader, top_model_full_opt, opt):\n","\n","  top_model = {'net': 0, 'num_epoch': 0, 'acc': 0}\n","  print(\"Starting (LR=\" + str(top_model_full_opt[\"lr\"]) + \", step size=\" + str(top_model_full_opt[\"step_size\"]) +  \", num max epoch: \" + str(top_model_full_opt[\"num_epoch\"]) + \").\")\n","\n","  net = alexnet(pretrained=True)\n","  net.classifier[6] = nn.Linear(4096, NUM_CLASSES) \n","\n","  criterion = nn.CrossEntropyLoss()\n","\n","  if opt == \"FC\":\n","    parameters_to_optimize = net.classifier.parameters()\n","    parameters_to_not_optimize = net.features.parameters()\n","    for p in parameters_to_not_optimize:\n","        p.requires_grad = False\n","  else:\n","    parameters_to_optimize = net.features.parameters()\n","    parameters_to_not_optimize = net.classifier.parameters()\n","    for p in parameters_to_not_optimize:\n","        p.requires_grad = False\n","\n","  optimizer = optim.SGD(parameters_to_optimize, lr=top_model_full_opt[\"lr\"], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=top_model_full_opt[\"step_size\"], gamma=GAMMA)\n","\n","  net = net.to(DEVICE)\n","\n","  for epoch in range(top_model_full_opt['num_epoch']):\n","\n","    ##############################\n","    #   Train Net on train+val   #\n","    ##############################\n","\n","    for images, labels in train_val_dataloader:\n","      \n","      images = images.to(DEVICE)\n","      labels = labels.to(DEVICE)\n","\n","      net.train()\n","      optimizer.zero_grad() # Zero-ing the gradients\n","\n","      outputs = net(images)\n","      loss = criterion(outputs, labels)\n","      if math.isnan(loss):\n","        print(\"           loss:\",loss)\n","\n","      loss.backward()  # backward pass: computes gradients\n","      optimizer.step() # update weights based on accumulated gradients\n","\n","    scheduler.step() \n","    print(\" Ending epoch \" + str(epoch)+ \"/\" + str(top_model_full_opt['num_epoch']) + \", LR = \", top_model_full_opt['lr'], \", step size = \", top_model_full_opt['step_size'])\n","\n","\n","\n","  #########################\n","  #   Evaluate Test Set   #\n","  #########################\n","\n","  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","  net.train(False) # Set Network to evaluation mode\n","\n","  running_corrects = 0\n","  for images, labels in tqdm(test_dataloader): \n","    images = images.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","\n","    # Forward Pass\n","    outputs = net(images)\n","\n","    # Get predictions\n","    _, preds = torch.max(outputs.data, 1)\n","\n","    # Update Corrects\n","    running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","  # Calculate Accuracy\n","  accuracy = running_corrects / float(len(test_dataloader.dataset))\n","\n","  print(\"Test Accuracy: \" + str(round(accuracy*100, 2)) + \"%.\" )\n","\n","class Caltech(VisionDataset):\n","\n","    def __init__(self, root, split='train', transform=None, target_transform=None):\n","\n","        super(Caltech, self).__init__(root, transform=transform, target_transform=target_transform)\n","\n","        IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n","    \n","        classes, class_to_idx = self._find_classes(self.root)\n","        samples = make_dataset(self.root, class_to_idx, IMG_EXTENSIONS)\n","\n","\n","        if len(samples) == 0:\n","            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n","                                \"Supported extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n","            \n","        self.split = split # This defines the split you are going to use\n","                           # (split files are called 'train.txt' and 'test.txt')\n","        self.loader =  default_loader\n","        self.extensions = IMG_EXTENSIONS\n","\n","        self.classes = classes\n","        self.class_to_idx = class_to_idx\n","        self.samples = samples\n","        self.targets = [s[1] for s in samples]\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def _find_classes(self, dir):\n","        if sys.version_info >= (3, 5):\n","            # Faster and available in Python 3.5 and above\n","            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n","        else:\n","            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n","        classes.sort()\n","        classes.remove(\"BACKGROUND_Google\")\n","        class_to_idx = {classes[i]: i for i in range(len(classes))}\n","        return classes, class_to_idx\n","\n","    def __getitem__(self, index):\n","   # Provide a way to access image and label via index. Image should be a PIL Image  label can be int\n","      path, target = self.samples[index]\n","      sample = self.loader(path)\n","      if self.transform is not None:\n","          sample = self.transform(sample)\n","      if self.target_transform is not None:\n","          target = self.target_transform(target)\n","\n","      return sample, target\n","\n","    def __len__(self):\n","        length = len # Provide a way to get the length (number of elements) of the dataset\n","        return len(self.samples)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9AOBZPb9IWvd","colab_type":"text"},"source":["**Punti 1, 2**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"louCvsGLIUrw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"08f9a4cd-df40-4e2d-f2f0-eb3380a2fee9","executionInfo":{"status":"ok","timestamp":1576412495622,"user_tz":-60,"elapsed":299639,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}}},"source":["%xmode Verbose\n","train_dataloader, val_dataloader, test_dataloader, train_val_dataloader = prepareDataloader(train_transform)\n","net = alexnet() \n","tuneHyperparameters(train_dataloader, val_dataloader, test_dataloader, train_val_dataloader, net)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","Starting (LR=0.1, step size=20).\n"," Ending epoch 1/10, LR = [0.1], step size = 20, Accuracy on validation set = 9.1978\n"," Ending epoch 2/10, LR = [0.1], step size = 20, Accuracy on validation set = 9.2324\n"," Ending epoch 3/10, LR = [0.1], step size = 20, Accuracy on validation set = 9.2324\n"," Ending epoch 4/10, LR = [0.1], step size = 20, Accuracy on validation set = 9.2324\n"," Ending epoch 5/10, LR = [0.1], step size = 20, Accuracy on validation set = 9.2324\n"," Ending epoch 6/10, LR = [0.1], step size = 20, Accuracy on validation set = 12.9668\n"," Ending epoch 7/10, LR = [0.1], step size = 20, Accuracy on validation set = 15.1452\n"," Ending epoch 8/10, LR = [0.1], step size = 20, Accuracy on validation set = 15.5602\n"," Ending epoch 9/10, LR = [0.1], step size = 20, Accuracy on validation set = 15.8022\n"," Ending epoch 10/10, LR = [0.1], step size = 20, Accuracy on validation set = 16.39\n","Best accuracy:  16.39 %. Epoch:  10 .\n","Starting (LR=0.01, step size=30).\n"," Ending epoch 1/10, LR = [0.01], step size = 30, Accuracy on validation set = 12.4136\n"," Ending epoch 2/10, LR = [0.01], step size = 30, Accuracy on validation set = 9.1978\n"," Ending epoch 3/10, LR = [0.01], step size = 30, Accuracy on validation set = 9.1978\n"," Ending epoch 4/10, LR = [0.01], step size = 30, Accuracy on validation set = 9.1978\n"," Ending epoch 5/10, LR = [0.01], step size = 30, Accuracy on validation set = 9.2324\n"," Ending epoch 6/10, LR = [0.01], step size = 30, Accuracy on validation set = 9.1978\n"," Ending epoch 7/10, LR = [0.01], step size = 30, Accuracy on validation set = 9.2669\n"," Ending epoch 8/10, LR = [0.01], step size = 30, Accuracy on validation set = 9.2324\n"," Ending epoch 9/10, LR = [0.01], step size = 30, Accuracy on validation set = 14.3845\n"," Ending epoch 10/10, LR = [0.01], step size = 30, Accuracy on validation set = 18.1535\n","Best accuracy:  18.1535 %. Epoch:  10 .\n","Starting (LR=0.01, step size=45).\n"," Ending epoch 1/10, LR = [0.01], step size = 45, Accuracy on validation set = 9.2324\n"," Ending epoch 2/10, LR = [0.01], step size = 45, Accuracy on validation set = 9.4398\n"," Ending epoch 3/10, LR = [0.01], step size = 45, Accuracy on validation set = 15.8714\n"," Ending epoch 4/10, LR = [0.01], step size = 45, Accuracy on validation set = 9.1978\n"," Ending epoch 5/10, LR = [0.01], step size = 45, Accuracy on validation set = 9.2324\n"," Ending epoch 6/10, LR = [0.01], step size = 45, Accuracy on validation set = 9.1978\n"," Ending epoch 7/10, LR = [0.01], step size = 45, Accuracy on validation set = 10.5118\n"," Ending epoch 8/10, LR = [0.01], step size = 45, Accuracy on validation set = 12.2407\n"," Ending epoch 9/10, LR = [0.01], step size = 45, Accuracy on validation set = 14.7649\n"," Ending epoch 10/10, LR = [0.01], step size = 45, Accuracy on validation set = 17.5311\n","Best accuracy:  17.5311 %. Epoch:  10 .\n","Starting (LR=0.001, step size=60).\n"," Ending epoch 1/10, LR = [0.001], step size = 60, Accuracy on validation set = 0.5187\n"," Ending epoch 2/10, LR = [0.001], step size = 60, Accuracy on validation set = 1.2448\n"," Ending epoch 3/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.2324\n"," Ending epoch 4/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.2324\n"," Ending epoch 5/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.2324\n"," Ending epoch 6/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.2324\n"," Ending epoch 7/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.2324\n"," Ending epoch 8/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.2324\n"," Ending epoch 9/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.2324\n"," Ending epoch 10/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.2324 Accuracy has not encreased, hp=4\n","Best accuracy:  9.2324 %. Epoch:  3 .\n","The set of the best hyperparameters is: [lr = 0.01, num_epoch = 10, step size = 30 ]. The best accuracy is: 18.15%.\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart train net on train+val\n"," Ending epoch 1/10, LR =  [0.01] , step size =  30\n"," Ending epoch 2/10, LR =  [0.01] , step size =  30\n"," Ending epoch 3/10, LR =  [0.01] , step size =  30\n"," Ending epoch 4/10, LR =  [0.01] , step size =  30\n"," Ending epoch 5/10, LR =  [0.01] , step size =  30\n"," Ending epoch 6/10, LR =  [0.01] , step size =  30\n"," Ending epoch 7/10, LR =  [0.01] , step size =  30\n"," Ending epoch 8/10, LR =  [0.01] , step size =  30\n"," Ending epoch 9/10, LR =  [0.01] , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/12 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 10/10, LR =  [0.01] , step size =  30\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart test net on test\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12/12 [00:05<00:00,  2.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 16.25%.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'acc': 0.18153526970954356, 'lr': 0.01, 'num_epoch': 10, 'step_size': 30}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"hnN9zTe2UThq","colab_type":"text"},"source":["**Transfert Learning**\n","Punto 3\n","\n"]},{"cell_type":"code","metadata":{"id":"Ca_WDs7PUTKr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6a389107-aea4-4e48-a5ab-9805d52f1ee9","executionInfo":{"status":"ok","timestamp":1576412994858,"user_tz":-60,"elapsed":798862,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}}},"source":["%xmode Verbose\n","#################\n","#   Punto 3.A   #\n","#################\n","net = alexnet(pretrained = True)\n","\n","\n","#################\n","#   Punto 3.B   #\n","#################\n","train_dataloader_TL, val_dataloader_TL, test_dataloader_TL, train_val_dataloader_TL = prepareDataloader(TL_transform)\n","\n","\n","#################\n","#   Punto 3.C   #\n","#################\n","print(\"#################\")\n","print(\"#   Punto 3.C   #\")\n","print(\"#################\")\n","top_model_full_opt = tuneHyperparameters(train_dataloader_TL, val_dataloader_TL, test_dataloader_TL, train_val_dataloader_TL, net)\n","\n","\n","#################\n","#   Punto 3.D   #\n","#################\n","print(\"#################\")\n","print(\"#   Punto 3.D   #\")\n","print(\"#################\")\n","partialTrain(train_val_dataloader_TL, test_dataloader_TL, top_model_full_opt, \"FC\")\n","\n","\n","#################\n","#   Punto 3.E   #\n","#################\n","print(\"#################\")\n","print(\"#   Punto 3.E   #\")\n","print(\"#################\")\n","partialTrain(train_val_dataloader_TL, test_dataloader_TL, top_model_full_opt, \"CL\")"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/checkpoints/alexnet-owt-4df8aa71.pth\n","100%|██████████| 233M/233M [00:00<00:00, 360MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","#################\n","#   Punto 3.C   #\n","#################\n","Starting (LR=0.1, step size=20).\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n"," Ending epoch 1/10, LR = [0.1], step size = 20, Accuracy on validation set = 5.0138\n","Best accuracy:  5.0138 %. Epoch:  1 .\n","Starting (LR=0.01, step size=30).\n"," Ending epoch 1/10, LR = [0.01], step size = 30, Accuracy on validation set = 60.6155\n"," Ending epoch 2/10, LR = [0.01], step size = 30, Accuracy on validation set = 79.8409\n"," Ending epoch 3/10, LR = [0.01], step size = 30, Accuracy on validation set = 82.0194\n"," Ending epoch 4/10, LR = [0.01], step size = 30, Accuracy on validation set = 83.0913\n"," Ending epoch 5/10, LR = [0.01], step size = 30, Accuracy on validation set = 82.8147\n"," Ending epoch 6/10, LR = [0.01], step size = 30, Accuracy on validation set = 81.3278\n"," Ending epoch 7/10, LR = [0.01], step size = 30, Accuracy on validation set = 83.7483\n"," Ending epoch 8/10, LR = [0.01], step size = 30, Accuracy on validation set = 83.6445\n"," Ending epoch 9/10, LR = [0.01], step size = 30, Accuracy on validation set = 84.2669\n"," Ending epoch 10/10, LR = [0.01], step size = 30, Accuracy on validation set = 85.166\n","Best accuracy:  85.166 %. Epoch:  10 .\n","Starting (LR=0.01, step size=45).\n"," Ending epoch 1/10, LR = [0.01], step size = 45, Accuracy on validation set = 63.1397\n"," Ending epoch 2/10, LR = [0.01], step size = 45, Accuracy on validation set = 77.9737\n"," Ending epoch 3/10, LR = [0.01], step size = 45, Accuracy on validation set = 80.4288\n"," Ending epoch 4/10, LR = [0.01], step size = 45, Accuracy on validation set = 80.7054\n"," Ending epoch 5/10, LR = [0.01], step size = 45, Accuracy on validation set = 81.7082\n"," Ending epoch 6/10, LR = [0.01], step size = 45, Accuracy on validation set = 83.2296\n"," Ending epoch 7/10, LR = [0.01], step size = 45, Accuracy on validation set = 83.2988\n"," Ending epoch 8/10, LR = [0.01], step size = 45, Accuracy on validation set = 84.4053\n"," Ending epoch 9/10, LR = [0.01], step size = 45, Accuracy on validation set = 83.7137\n"," Ending epoch 10/10, LR = [0.01], step size = 45, Accuracy on validation set = 84.7165\n","Best accuracy:  84.7165 %. Epoch:  10 .\n","Starting (LR=0.001, step size=60).\n"," Ending epoch 1/10, LR = [0.001], step size = 60, Accuracy on validation set = 30.325\n"," Ending epoch 2/10, LR = [0.001], step size = 60, Accuracy on validation set = 48.0636\n"," Ending epoch 3/10, LR = [0.001], step size = 60, Accuracy on validation set = 60.0968\n"," Ending epoch 4/10, LR = [0.001], step size = 60, Accuracy on validation set = 69.9862\n"," Ending epoch 5/10, LR = [0.001], step size = 60, Accuracy on validation set = 76.4177\n"," Ending epoch 6/10, LR = [0.001], step size = 60, Accuracy on validation set = 79.7026\n"," Ending epoch 7/10, LR = [0.001], step size = 60, Accuracy on validation set = 80.7746\n"," Ending epoch 8/10, LR = [0.001], step size = 60, Accuracy on validation set = 81.5698\n"," Ending epoch 9/10, LR = [0.001], step size = 60, Accuracy on validation set = 81.3624\n"," Ending epoch 10/10, LR = [0.001], step size = 60, Accuracy on validation set = 81.6736\n","Best accuracy:  81.6736 %. Epoch:  10 .\n","The set of the best hyperparameters is: [lr = 0.01, num_epoch = 10, step size = 30 ]. The best accuracy is: 85.17%.\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart train net on train+val\n"," Ending epoch 1/10, LR =  [0.01] , step size =  30\n"," Ending epoch 2/10, LR =  [0.01] , step size =  30\n"," Ending epoch 3/10, LR =  [0.01] , step size =  30\n"," Ending epoch 4/10, LR =  [0.01] , step size =  30\n"," Ending epoch 5/10, LR =  [0.01] , step size =  30\n"," Ending epoch 6/10, LR =  [0.01] , step size =  30\n"," Ending epoch 7/10, LR =  [0.01] , step size =  30\n"," Ending epoch 8/10, LR =  [0.01] , step size =  30\n"," Ending epoch 9/10, LR =  [0.01] , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/12 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 10/10, LR =  [0.01] , step size =  30\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart test net on test\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12/12 [00:07<00:00,  1.69it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 82.86%.\n","#################\n","#   Punto 3.D   #\n","#################\n","Starting (LR=0.01, step size=30, num max epoch: 10).\n"," Ending epoch 0/10, LR =  0.01 , step size =  30\n"," Ending epoch 1/10, LR =  0.01 , step size =  30\n"," Ending epoch 2/10, LR =  0.01 , step size =  30\n"," Ending epoch 3/10, LR =  0.01 , step size =  30\n"," Ending epoch 4/10, LR =  0.01 , step size =  30\n"," Ending epoch 5/10, LR =  0.01 , step size =  30\n"," Ending epoch 6/10, LR =  0.01 , step size =  30\n"," Ending epoch 7/10, LR =  0.01 , step size =  30\n"," Ending epoch 8/10, LR =  0.01 , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/12 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 9/10, LR =  0.01 , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12/12 [00:06<00:00,  1.74it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 86.86%.\n","#################\n","#   Punto 3.E   #\n","#################\n","Starting (LR=0.01, step size=30, num max epoch: 10).\n"," Ending epoch 0/10, LR =  0.01 , step size =  30\n"," Ending epoch 1/10, LR =  0.01 , step size =  30\n"," Ending epoch 2/10, LR =  0.01 , step size =  30\n"," Ending epoch 3/10, LR =  0.01 , step size =  30\n"," Ending epoch 4/10, LR =  0.01 , step size =  30\n"," Ending epoch 5/10, LR =  0.01 , step size =  30\n"," Ending epoch 6/10, LR =  0.01 , step size =  30\n"," Ending epoch 7/10, LR =  0.01 , step size =  30\n"," Ending epoch 8/10, LR =  0.01 , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/12 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 9/10, LR =  0.01 , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12/12 [00:07<00:00,  1.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 54.27%.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"VXLhAJ8jSJ0E","colab_type":"text"},"source":["**Punto 4**"]},{"cell_type":"code","metadata":{"id":"OiuRqfbUSKKT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2f8334f2-177b-410d-d4ef-0d642c658337","executionInfo":{"status":"ok","timestamp":1576413738155,"user_tz":-60,"elapsed":1542155,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}}},"source":["\n","%xmode Verbose\n","train_dataloader_DA_1, val_dataloader_DA_1, test_dataloader_DA_1, train_val_dataloader_DA_1 = prepareDataloader(data_augmentation_transform_1)\n","train_dataloader_DA_2, val_dataloader_DA_2, test_dataloader_DA_2, train_val_dataloader_DA_2 = prepareDataloader(data_augmentation_transform_2)\n","train_dataloader_DA_3, val_dataloader_DA_3, test_dataloader_DA_3, train_val_dataloader_DA_3 = prepareDataloader(data_augmentation_transform_3)\n","\n","net = alexnet(pretrained = True)\n","\n","print(\"\\n----------------------------------------------------------------------\\n\")\n","print(\"First DA\")\n","tuneHyperparameters(train_dataloader_DA_1, val_dataloader, test_dataloader, train_val_dataloader_DA_1, net) \n","print(\"\\n----------------------------------------------------------------------\\n\")\n","print(\"Second DA\")\n","tuneHyperparameters(train_dataloader_DA_2, val_dataloader, test_dataloader, train_val_dataloader_DA_2, net) \n","print(\"\\n----------------------------------------------------------------------\\n\")\n","print(\"Third DA\")\n","tuneHyperparameters(train_dataloader_DA_3, val_dataloader, test_dataloader, train_val_dataloader_DA_3, net) \n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","\n","----------------------------------------------------------------------\n","\n","First DA\n","Starting (LR=0.1, step size=20).\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n"," Ending epoch 1/10, LR = [0.1], step size = 20, Accuracy on validation set = 5.0138\n","Best accuracy:  5.0138 %. Epoch:  1 .\n","Starting (LR=0.01, step size=30).\n"," Ending epoch 1/10, LR = [0.01], step size = 30, Accuracy on validation set = 50.7607\n"," Ending epoch 2/10, LR = [0.01], step size = 30, Accuracy on validation set = 69.3292\n"," Ending epoch 3/10, LR = [0.01], step size = 30, Accuracy on validation set = 71.4385\n"," Ending epoch 4/10, LR = [0.01], step size = 30, Accuracy on validation set = 72.2683\n"," Ending epoch 5/10, LR = [0.01], step size = 30, Accuracy on validation set = 74.2393\n"," Ending epoch 6/10, LR = [0.01], step size = 30, Accuracy on validation set = 73.1328\n"," Ending epoch 7/10, LR = [0.01], step size = 30, Accuracy on validation set = 72.4412\n"," Ending epoch 8/10, LR = [0.01], step size = 30, Accuracy on validation set = 72.5795\n"," Ending epoch 9/10, LR = [0.01], step size = 30, Accuracy on validation set = 75.0692\n"," Ending epoch 10/10, LR = [0.01], step size = 30, Accuracy on validation set = 75.7607\n","Best accuracy:  75.7607 %. Epoch:  10 .\n","Starting (LR=0.01, step size=45).\n"," Ending epoch 1/10, LR = [0.01], step size = 45, Accuracy on validation set = 53.5961\n"," Ending epoch 2/10, LR = [0.01], step size = 45, Accuracy on validation set = 67.8769\n"," Ending epoch 3/10, LR = [0.01], step size = 45, Accuracy on validation set = 67.9806\n"," Ending epoch 4/10, LR = [0.01], step size = 45, Accuracy on validation set = 69.675\n"," Ending epoch 5/10, LR = [0.01], step size = 45, Accuracy on validation set = 73.3057\n"," Ending epoch 6/10, LR = [0.01], step size = 45, Accuracy on validation set = 74.2739\n"," Ending epoch 7/10, LR = [0.01], step size = 45, Accuracy on validation set = 73.1328\n"," Ending epoch 8/10, LR = [0.01], step size = 45, Accuracy on validation set = 68.361\n"," Ending epoch 9/10, LR = [0.01], step size = 45, Accuracy on validation set = 72.0263\n"," Ending epoch 10/10, LR = [0.01], step size = 45, Accuracy on validation set = 73.0636 Accuracy has not encreased, hp=4\n","Best accuracy:  74.2739 %. Epoch:  6 .\n","Starting (LR=0.001, step size=60).\n"," Ending epoch 1/10, LR = [0.001], step size = 60, Accuracy on validation set = 28.1812\n"," Ending epoch 2/10, LR = [0.001], step size = 60, Accuracy on validation set = 39.0387\n"," Ending epoch 3/10, LR = [0.001], step size = 60, Accuracy on validation set = 48.9972\n"," Ending epoch 4/10, LR = [0.001], step size = 60, Accuracy on validation set = 56.3278\n"," Ending epoch 5/10, LR = [0.001], step size = 60, Accuracy on validation set = 61.7566\n"," Ending epoch 6/10, LR = [0.001], step size = 60, Accuracy on validation set = 65.4219\n"," Ending epoch 7/10, LR = [0.001], step size = 60, Accuracy on validation set = 69.3292\n"," Ending epoch 8/10, LR = [0.001], step size = 60, Accuracy on validation set = 69.675\n"," Ending epoch 9/10, LR = [0.001], step size = 60, Accuracy on validation set = 69.9516\n"," Ending epoch 10/10, LR = [0.001], step size = 60, Accuracy on validation set = 69.8479\n","Best accuracy:  69.9516 %. Epoch:  9 .\n","The set of the best hyperparameters is: [lr = 0.01, num_epoch = 10, step size = 30 ]. The best accuracy is: 75.76%.\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart train net on train+val\n"," Ending epoch 1/10, LR =  [0.01] , step size =  30\n"," Ending epoch 2/10, LR =  [0.01] , step size =  30\n"," Ending epoch 3/10, LR =  [0.01] , step size =  30\n"," Ending epoch 4/10, LR =  [0.01] , step size =  30\n"," Ending epoch 5/10, LR =  [0.01] , step size =  30\n"," Ending epoch 6/10, LR =  [0.01] , step size =  30\n"," Ending epoch 7/10, LR =  [0.01] , step size =  30\n"," Ending epoch 8/10, LR =  [0.01] , step size =  30\n"," Ending epoch 9/10, LR =  [0.01] , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/12 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 10/10, LR =  [0.01] , step size =  30\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart test net on test\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12/12 [00:06<00:00,  1.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 73.94%.\n","\n","----------------------------------------------------------------------\n","\n","Second DA\n","Starting (LR=0.1, step size=20).\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n"," Ending epoch 1/10, LR = [0.1], step size = 20, Accuracy on validation set = 5.0138\n","Best accuracy:  5.0138 %. Epoch:  1 .\n","Starting (LR=0.01, step size=30).\n"," Ending epoch 1/10, LR = [0.01], step size = 30, Accuracy on validation set = 46.0235\n"," Ending epoch 2/10, LR = [0.01], step size = 30, Accuracy on validation set = 59.1632\n"," Ending epoch 3/10, LR = [0.01], step size = 30, Accuracy on validation set = 68.7068\n"," Ending epoch 4/10, LR = [0.01], step size = 30, Accuracy on validation set = 66.8741\n"," Ending epoch 5/10, LR = [0.01], step size = 30, Accuracy on validation set = 66.0788\n"," Ending epoch 6/10, LR = [0.01], step size = 30, Accuracy on validation set = 68.603\n"," Ending epoch 7/10, LR = [0.01], step size = 30, Accuracy on validation set = 66.7358\n"," Ending epoch 8/10, LR = [0.01], step size = 30, Accuracy on validation set = 70.1245\n"," Ending epoch 9/10, LR = [0.01], step size = 30, Accuracy on validation set = 67.6003\n"," Ending epoch 10/10, LR = [0.01], step size = 30, Accuracy on validation set = 66.9779\n","Best accuracy:  70.1245 %. Epoch:  8 .\n","Starting (LR=0.01, step size=45).\n"," Ending epoch 1/10, LR = [0.01], step size = 45, Accuracy on validation set = 43.4647\n"," Ending epoch 2/10, LR = [0.01], step size = 45, Accuracy on validation set = 61.8949\n"," Ending epoch 3/10, LR = [0.01], step size = 45, Accuracy on validation set = 63.1743\n"," Ending epoch 4/10, LR = [0.01], step size = 45, Accuracy on validation set = 62.8976\n"," Ending epoch 5/10, LR = [0.01], step size = 45, Accuracy on validation set = 64.177\n"," Ending epoch 6/10, LR = [0.01], step size = 45, Accuracy on validation set = 66.1134\n"," Ending epoch 7/10, LR = [0.01], step size = 45, Accuracy on validation set = 69.0526\n"," Ending epoch 8/10, LR = [0.01], step size = 45, Accuracy on validation set = 65.8714\n"," Ending epoch 9/10, LR = [0.01], step size = 45, Accuracy on validation set = 68.603\n"," Ending epoch 10/10, LR = [0.01], step size = 45, Accuracy on validation set = 68.8105\n","Best accuracy:  69.0526 %. Epoch:  7 .\n","Starting (LR=0.001, step size=60).\n"," Ending epoch 1/10, LR = [0.001], step size = 60, Accuracy on validation set = 26.6252\n"," Ending epoch 2/10, LR = [0.001], step size = 60, Accuracy on validation set = 35.6846\n"," Ending epoch 3/10, LR = [0.001], step size = 60, Accuracy on validation set = 44.5712\n"," Ending epoch 4/10, LR = [0.001], step size = 60, Accuracy on validation set = 52.3513\n"," Ending epoch 5/10, LR = [0.001], step size = 60, Accuracy on validation set = 56.6044\n"," Ending epoch 6/10, LR = [0.001], step size = 60, Accuracy on validation set = 59.7165\n"," Ending epoch 7/10, LR = [0.001], step size = 60, Accuracy on validation set = 62.5864\n"," Ending epoch 8/10, LR = [0.001], step size = 60, Accuracy on validation set = 63.6929\n"," Ending epoch 9/10, LR = [0.001], step size = 60, Accuracy on validation set = 63.2089\n"," Ending epoch 10/10, LR = [0.001], step size = 60, Accuracy on validation set = 64.6957\n","Best accuracy:  64.6957 %. Epoch:  10 .\n","The set of the best hyperparameters is: [lr = 0.01, num_epoch = 8, step size = 30 ]. The best accuracy is: 70.12%.\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart train net on train+val\n"," Ending epoch 1/8, LR =  [0.01] , step size =  30\n"," Ending epoch 2/8, LR =  [0.01] , step size =  30\n"," Ending epoch 3/8, LR =  [0.01] , step size =  30\n"," Ending epoch 4/8, LR =  [0.01] , step size =  30\n"," Ending epoch 5/8, LR =  [0.01] , step size =  30\n"," Ending epoch 6/8, LR =  [0.01] , step size =  30\n"," Ending epoch 7/8, LR =  [0.01] , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/12 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 8/8, LR =  [0.01] , step size =  30\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart test net on test\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12/12 [00:06<00:00,  1.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 68.99%.\n","\n","----------------------------------------------------------------------\n","\n","Third DA\n","Starting (LR=0.1, step size=20).\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n"," Ending epoch 1/10, LR = [0.1], step size = 20, Accuracy on validation set = 5.0138\n","Best accuracy:  5.0138 %. Epoch:  1 .\n","Starting (LR=0.01, step size=30).\n"," Ending epoch 1/10, LR = [0.01], step size = 30, Accuracy on validation set = 51.9364\n"," Ending epoch 2/10, LR = [0.01], step size = 30, Accuracy on validation set = 74.2739\n"," Ending epoch 3/10, LR = [0.01], step size = 30, Accuracy on validation set = 76.4869\n"," Ending epoch 4/10, LR = [0.01], step size = 30, Accuracy on validation set = 71.5076\n"," Ending epoch 5/10, LR = [0.01], step size = 30, Accuracy on validation set = 71.1618\n"," Ending epoch 6/10, LR = [0.01], step size = 30, Accuracy on validation set = 72.8216\n"," Ending epoch 7/10, LR = [0.01], step size = 30, Accuracy on validation set = 73.4094\n"," Ending epoch 8/10, LR = [0.01], step size = 30, Accuracy on validation set = 78.3887\n"," Ending epoch 9/10, LR = [0.01], step size = 30, Accuracy on validation set = 76.6943\n"," Ending epoch 10/10, LR = [0.01], step size = 30, Accuracy on validation set = 76.8326\n","Best accuracy:  78.3887 %. Epoch:  8 .\n","Starting (LR=0.01, step size=45).\n"," Ending epoch 1/10, LR = [0.01], step size = 45, Accuracy on validation set = 57.3651\n"," Ending epoch 2/10, LR = [0.01], step size = 45, Accuracy on validation set = 70.5394\n"," Ending epoch 3/10, LR = [0.01], step size = 45, Accuracy on validation set = 74.9654\n"," Ending epoch 4/10, LR = [0.01], step size = 45, Accuracy on validation set = 72.9599\n"," Ending epoch 5/10, LR = [0.01], step size = 45, Accuracy on validation set = 71.7497\n"," Ending epoch 6/10, LR = [0.01], step size = 45, Accuracy on validation set = 74.5851\n"," Ending epoch 7/10, LR = [0.01], step size = 45, Accuracy on validation set = 72.5795\n"," Ending epoch 8/10, LR = [0.01], step size = 45, Accuracy on validation set = 73.8243\n"," Ending epoch 9/10, LR = [0.01], step size = 45, Accuracy on validation set = 74.1701\n"," Ending epoch 10/10, LR = [0.01], step size = 45, Accuracy on validation set = 75.0\n","Best accuracy:  75.0 %. Epoch:  10 .\n","Starting (LR=0.001, step size=60).\n"," Ending epoch 1/10, LR = [0.001], step size = 60, Accuracy on validation set = 27.6625\n"," Ending epoch 2/10, LR = [0.001], step size = 60, Accuracy on validation set = 41.148\n"," Ending epoch 3/10, LR = [0.001], step size = 60, Accuracy on validation set = 52.9737\n"," Ending epoch 4/10, LR = [0.001], step size = 60, Accuracy on validation set = 61.6183\n"," Ending epoch 5/10, LR = [0.001], step size = 60, Accuracy on validation set = 66.2863\n"," Ending epoch 6/10, LR = [0.001], step size = 60, Accuracy on validation set = 70.574\n"," Ending epoch 7/10, LR = [0.001], step size = 60, Accuracy on validation set = 72.8216\n"," Ending epoch 8/10, LR = [0.001], step size = 60, Accuracy on validation set = 73.6515\n"," Ending epoch 9/10, LR = [0.001], step size = 60, Accuracy on validation set = 73.6515\n"," Ending epoch 10/10, LR = [0.001], step size = 60, Accuracy on validation set = 74.2047\n","Best accuracy:  74.2047 %. Epoch:  10 .\n","The set of the best hyperparameters is: [lr = 0.01, num_epoch = 8, step size = 30 ]. The best accuracy is: 78.39%.\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart train net on train+val\n"," Ending epoch 1/8, LR =  [0.01] , step size =  30\n"," Ending epoch 2/8, LR =  [0.01] , step size =  30\n"," Ending epoch 3/8, LR =  [0.01] , step size =  30\n"," Ending epoch 4/8, LR =  [0.01] , step size =  30\n"," Ending epoch 5/8, LR =  [0.01] , step size =  30\n"," Ending epoch 6/8, LR =  [0.01] , step size =  30\n"," Ending epoch 7/8, LR =  [0.01] , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/12 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 8/8, LR =  [0.01] , step size =  30\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart test net on test\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12/12 [00:06<00:00,  1.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 76.32%.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'acc': 0.7838865836791148, 'lr': 0.01, 'num_epoch': 8, 'step_size': 30}"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"y1cDK-yFCQeq","colab_type":"text"},"source":["Punto 5\n"]},{"cell_type":"code","metadata":{"id":"7CJlF8rvCSoB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"dba15bb7-58f0-462f-cbca-3151d473ea2e"},"source":["%xmode Verbose\n","\n","BATCH_SIZE = 32\n","print(\"#################\")\n","print(\"# Punto 5.1/5.2 #\")\n","print(\"#################\")\n","\n","net = vgg11() \n","train_dataloader, val_dataloader, test_dataloader, train_val_dataloader = prepareDataloader(train_transform)\n","tuneHyperparameters(train_dataloader, val_dataloader, test_dataloader, train_val_dataloader, net)\n","\n","\n","#################\n","#   Punto 5.3   #\n","#################\n","\n","print(\"#################\")\n","print(\"#   Punto 5.3   #\")\n","print(\"#################\")\n","\n","net = vgg11(pretrained = True)\n","train_dataloader_TL, val_dataloader_TL, test_dataloader_TL, train_val_dataloader_TL = prepareDataloader(TL_transform)\n","top_model_full_opt = tuneHyperparameters(train_dataloader_TL, val_dataloader_TL, test_dataloader_TL, train_val_dataloader_TL, net)\n","\n","\n","#################\n","#   Punto 5.4   #\n","#################\n","\n","print(\"#################\")\n","print(\"#   Punto 5.4   #\")\n","print(\"#################\")\n","\n","train_dataloader_DA_1, val_dataloader_DA_1, test_dataloader_DA_1, train_val_dataloader_DA_1 = prepareDataloader(data_augmentation_transform_1)\n","train_dataloader_DA_2, val_dataloader_DA_2, test_dataloader_DA_2, train_val_dataloader_DA_2 = prepareDataloader(data_augmentation_transform_2)\n","train_dataloader_DA_3, val_dataloader_DA_3, test_dataloader_DA_3, train_val_dataloader_DA_3 = prepareDataloader(data_augmentation_transform_3)\n","\n","print(\"\\n----------------------------------------------------------------------\\n\")\n","print(\"First DA\")\n","tuneHyperparameters(train_dataloader_DA_1, val_dataloader, test_dataloader, train_val_dataloader_DA_1, net) \n","print(\"\\n----------------------------------------------------------------------\\n\")\n","print(\"Second DA\")\n","tuneHyperparameters(train_dataloader_DA_2, val_dataloader, test_dataloader, train_val_dataloader_DA_2, net) \n","print(\"\\n----------------------------------------------------------------------\\n\")\n","print(\"Third DA\")\n","tuneHyperparameters(train_dataloader_DA_3, val_dataloader, test_dataloader, train_val_dataloader_DA_3, net) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n","#################\n","# Punto 5.1/5.2 #\n","#################\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","Starting (LR=0.1, step size=20).\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n"," Ending epoch 1/10, LR = [0.1], step size = 20, Accuracy on validation set = 5.0138\n","Best accuracy:  5.0138 %. Epoch:  1 .\n","Starting (LR=0.01, step size=30).\n"," Ending epoch 1/10, LR = [0.01], step size = 30, Accuracy on validation set = 21.1964\n"," Ending epoch 2/10, LR = [0.01], step size = 30, Accuracy on validation set = 24.9654\n"," Ending epoch 3/10, LR = [0.01], step size = 30, Accuracy on validation set = 29.2531\n"," Ending epoch 4/10, LR = [0.01], step size = 30, Accuracy on validation set = 34.5781\n"," Ending epoch 5/10, LR = [0.01], step size = 30, Accuracy on validation set = 36.7566\n"," Ending epoch 6/10, LR = [0.01], step size = 30, Accuracy on validation set = 39.6266\n"," Ending epoch 7/10, LR = [0.01], step size = 30, Accuracy on validation set = 41.4592\n"," Ending epoch 8/10, LR = [0.01], step size = 30, Accuracy on validation set = 44.0871\n"," Ending epoch 9/10, LR = [0.01], step size = 30, Accuracy on validation set = 46.6113\n"," Ending epoch 10/10, LR = [0.01], step size = 30, Accuracy on validation set = 43.9142\n","Best accuracy:  46.6113 %. Epoch:  9 .\n","Starting (LR=0.01, step size=45).\n"," Ending epoch 1/10, LR = [0.01], step size = 45, Accuracy on validation set = 21.4385\n"," Ending epoch 2/10, LR = [0.01], step size = 45, Accuracy on validation set = 24.3776\n"," Ending epoch 3/10, LR = [0.01], step size = 45, Accuracy on validation set = 30.325\n"," Ending epoch 4/10, LR = [0.01], step size = 45, Accuracy on validation set = 33.1604\n"," Ending epoch 5/10, LR = [0.01], step size = 45, Accuracy on validation set = 35.5463\n"," Ending epoch 6/10, LR = [0.01], step size = 45, Accuracy on validation set = 39.3154\n"," Ending epoch 7/10, LR = [0.01], step size = 45, Accuracy on validation set = 38.8313\n"," Ending epoch 8/10, LR = [0.01], step size = 45, Accuracy on validation set = 39.834\n"," Ending epoch 9/10, LR = [0.01], step size = 45, Accuracy on validation set = 44.917\n"," Ending epoch 10/10, LR = [0.01], step size = 45, Accuracy on validation set = 46.4039\n","Best accuracy:  46.4039 %. Epoch:  10 .\n","Starting (LR=0.001, step size=60).\n"," Ending epoch 1/10, LR = [0.001], step size = 60, Accuracy on validation set = 9.1978\n"," Ending epoch 2/10, LR = [0.001], step size = 60, Accuracy on validation set = 20.9544\n"," Ending epoch 3/10, LR = [0.001], step size = 60, Accuracy on validation set = 21.9571\n"," Ending epoch 4/10, LR = [0.001], step size = 60, Accuracy on validation set = 24.9654\n"," Ending epoch 5/10, LR = [0.001], step size = 60, Accuracy on validation set = 29.8409\n"," Ending epoch 6/10, LR = [0.001], step size = 60, Accuracy on validation set = 31.0858\n"," Ending epoch 7/10, LR = [0.001], step size = 60, Accuracy on validation set = 32.4343\n"," Ending epoch 8/10, LR = [0.001], step size = 60, Accuracy on validation set = 34.6473\n"," Ending epoch 9/10, LR = [0.001], step size = 60, Accuracy on validation set = 36.7566\n"," Ending epoch 10/10, LR = [0.001], step size = 60, Accuracy on validation set = 38.1051\n","Best accuracy:  38.1051 %. Epoch:  10 .\n","The set of the best hyperparameters is: [lr = 0.01, num_epoch = 9, step size = 30 ]. The best accuracy is: 46.61%.\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart train net on train+val\n"," Ending epoch 1/9, LR =  [0.01] , step size =  30\n"," Ending epoch 2/9, LR =  [0.01] , step size =  30\n"," Ending epoch 3/9, LR =  [0.01] , step size =  30\n"," Ending epoch 4/9, LR =  [0.01] , step size =  30\n"," Ending epoch 5/9, LR =  [0.01] , step size =  30\n"," Ending epoch 6/9, LR =  [0.01] , step size =  30\n"," Ending epoch 7/9, LR =  [0.01] , step size =  30\n"," Ending epoch 8/9, LR =  [0.01] , step size =  30\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/91 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 9/9, LR =  [0.01] , step size =  30\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart test net on test\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 91/91 [00:06<00:00, 13.79it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 44.87%.\n","#################\n","#   Punto 5.3   #\n","#################\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","Starting (LR=0.1, step size=20).\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n"," Ending epoch 1/10, LR = [0.1], step size = 20, Accuracy on validation set = 5.0138\n","Best accuracy:  5.0138 %. Epoch:  1 .\n","Starting (LR=0.01, step size=30).\n"," Ending epoch 1/10, LR = [0.01], step size = 30, Accuracy on validation set = 57.953\n"," Ending epoch 2/10, LR = [0.01], step size = 30, Accuracy on validation set = 72.3721\n"," Ending epoch 3/10, LR = [0.01], step size = 30, Accuracy on validation set = 73.9627\n"," Ending epoch 4/10, LR = [0.01], step size = 30, Accuracy on validation set = 79.3914\n"," Ending epoch 5/10, LR = [0.01], step size = 30, Accuracy on validation set = 80.4633\n"," Ending epoch 6/10, LR = [0.01], step size = 30, Accuracy on validation set = 80.3596\n"," Ending epoch 7/10, LR = [0.01], step size = 30, Accuracy on validation set = 76.0028\n"," Ending epoch 8/10, LR = [0.01], step size = 30, Accuracy on validation set = 77.4896\n"," Ending epoch 9/10, LR = [0.01], step size = 30, Accuracy on validation set = 80.8437\n"," Ending epoch 10/10, LR = [0.01], step size = 30, Accuracy on validation set = 85.0968\n","Best accuracy:  85.0968 %. Epoch:  10 .\n","Starting (LR=0.01, step size=45).\n"," Ending epoch 1/10, LR = [0.01], step size = 45, Accuracy on validation set = 61.1687\n"," Ending epoch 2/10, LR = [0.01], step size = 45, Accuracy on validation set = 73.2365\n"," Ending epoch 3/10, LR = [0.01], step size = 45, Accuracy on validation set = 75.4149\n"," Ending epoch 4/10, LR = [0.01], step size = 45, Accuracy on validation set = 78.4924\n"," Ending epoch 5/10, LR = [0.01], step size = 45, Accuracy on validation set = 78.6999\n"," Ending epoch 6/10, LR = [0.01], step size = 45, Accuracy on validation set = 82.8838\n"," Ending epoch 7/10, LR = [0.01], step size = 45, Accuracy on validation set = 83.195\n"," Ending epoch 8/10, LR = [0.01], step size = 45, Accuracy on validation set = 79.184\n"," Ending epoch 9/10, LR = [0.01], step size = 45, Accuracy on validation set = 79.5989\n"," Ending epoch 10/10, LR = [0.01], step size = 45, Accuracy on validation set = 80.2559\n","Best accuracy:  83.195 %. Epoch:  7 .\n","Starting (LR=0.001, step size=60).\n"," Ending epoch 1/10, LR = [0.001], step size = 60, Accuracy on validation set = 80.4633\n"," Ending epoch 2/10, LR = [0.001], step size = 60, Accuracy on validation set = 85.823\n"," Ending epoch 3/10, LR = [0.001], step size = 60, Accuracy on validation set = 86.7911\n"," Ending epoch 4/10, LR = [0.001], step size = 60, Accuracy on validation set = 88.7967\n"," Ending epoch 5/10, LR = [0.001], step size = 60, Accuracy on validation set = 89.9378\n"," Ending epoch 6/10, LR = [0.001], step size = 60, Accuracy on validation set = 90.3181\n"," Ending epoch 7/10, LR = [0.001], step size = 60, Accuracy on validation set = 89.9032\n"," Ending epoch 8/10, LR = [0.001], step size = 60, Accuracy on validation set = 90.8368\n"," Ending epoch 9/10, LR = [0.001], step size = 60, Accuracy on validation set = 90.7676\n"," Ending epoch 10/10, LR = [0.001], step size = 60, Accuracy on validation set = 90.5256\n","Best accuracy:  90.8368 %. Epoch:  8 .\n","The set of the best hyperparameters is: [lr = 0.001, num_epoch = 8, step size = 60 ]. The best accuracy is: 90.84%.\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart train net on train+val\n"," Ending epoch 1/8, LR =  [0.001] , step size =  60\n"," Ending epoch 2/8, LR =  [0.001] , step size =  60\n"," Ending epoch 3/8, LR =  [0.001] , step size =  60\n"," Ending epoch 4/8, LR =  [0.001] , step size =  60\n"," Ending epoch 5/8, LR =  [0.001] , step size =  60\n"," Ending epoch 6/8, LR =  [0.001] , step size =  60\n"," Ending epoch 7/8, LR =  [0.001] , step size =  60\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/91 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":[" Ending epoch 8/8, LR =  [0.001] , step size =  60\n","----------------------------------------------------------------------------------------------------------------------------------------------\n","Strart test net on test\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 91/91 [00:06<00:00, 13.47it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy: 88.84%.\n","#################\n","#   Punto 5.4   #\n","#################\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","Train Dataset: 2892\n","Val Dataset: 2892\n","Test Dataset: 2893\n","Train + Val dataset: 5784\n","\n","----------------------------------------------------------------------\n","\n","First DA\n","Starting (LR=0.1, step size=20).\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n","           loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n"," Ending epoch 1/10, LR = [0.1], step size = 20, Accuracy on validation set = 5.0138\n","Best accuracy:  5.0138 %. Epoch:  1 .\n","Starting (LR=0.01, step size=30).\n"],"name":"stdout"}]}]}